<div align="center">
<h1>Local All-Pair Correspondence for Point Tracking</h1>

[**Seokju Cho**](https://seokju-cho.github.io)<sup>1</sup> Â· [**Jiahui Huang**](https://gabriel-huang.github.io)<sup>2</sup> Â· [**Jisu Nam**](https://nam-jisu.github.io)<sup>1</sup> Â· [**Honggyu An**](https://hg010303.github.io)<sup>1</sup> Â· [**Seungryong Kim**](https://cvlab.korea.ac.kr)<sup>1</sup> Â· [**Joon-Young Lee**](https://joonyoung-cv.github.io)<sup>2</sup>

<sup>1</sup>Korea University&emsp;&emsp;&emsp;&emsp;<sup>2</sup>Adobe Research

**ECCV 2024**

<a href="https://arxiv.org/abs/2407.15420"><img src='https://img.shields.io/badge/arXiv-LocoTrack-red' alt='Paper PDF'></a>
<a href='https://ku-cvlab.github.io/locotrack'><img src='https://img.shields.io/badge/Project_Page-LocoTrack-green' alt='Project Page'></a>
<a href='https://huggingface.co/spaces/hamacojr/LocoTrack'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue'></a>

<p float='center'><img src="assets/teaser.png" width="80%" /></p>
<span style="color: green; font-size: 1.3em; font-weight: bold;">LocoTrack is an incredibly efficient model,</span> enabling near-dense point tracking in real-time. It is <span style="color: red; font-size: 1.3em; font-weight: bold;">6x faster</span> than the previous state-of-the-art models.
</div>

---

## é¡¹ç›®æ”¹è¿›è¯´æ˜
æœ¬é¡¹ç›®åŸºäºåŸç‰ˆ LocoTrackï¼Œä¸»è¦å°†ç‰¹å¾æå–éƒ¨åˆ†æ›¿æ¢ä¸º EfficientNetï¼ˆç›®å‰å·²æœ‰çš„chekponitæ”¯æŒ b0ã€b2ã€b5 ç‰ˆæœ¬ï¼‰ï¼Œå¹¶åœ¨è°ƒæ•´äº†ç›¸å…³å‚æ•°åšäº†å¤šæ¬¡å®éªŒã€‚ç›¸æ¯”åŸå§‹å®ç°ï¼ŒEfficientNet èƒ½å¸¦æ¥æ›´ä¼˜çš„é€Ÿåº¦ä¸ç²¾åº¦æƒè¡¡ã€‚

---

## å…³é”®è„šæœ¬è¯´æ˜
*   **[combined_tracker_faceparsing_multi.py](https://github.com/zyp-up/locotrack-pytorch-efficientnet/blob/main/combined_tracker_faceparsing_multi.py)**ï¼šè¯¥è„šæœ¬ä¸ºä¸€ä¸ªé€šç”¨çš„äººè„¸è·Ÿè¸ªå™¨è„šæœ¬ï¼Œå¦‚æœéœ€è¦ä¿®æ”¹ï¼Œåªéœ€è¦å°†å¯¹åº”çš„æ£€æµ‹å™¨å’Œè·Ÿè¸ªå™¨æ¢æˆæ‚¨çš„ç‰¹å®šéœ€æ±‚ï¼Œå°±å¯ä»¥åœ¨ä»»ä½•è§†é¢‘åœºæ™¯å®ç°å¤šç›®æ ‡è·Ÿè¸ªã€‚

---

## Training and Evaluation
For detailed instructions on training and evaluation, please refer to the README file for your chosen implementation:

- **[JAX Implementation](./locotrack/README.md)**
- **[PyTorch Implementation](./locotrack_pytorch/README.md)**

## Evaluation Dataset Preparation
First, download the evaluation datasets:
```bash
# TAP-Vid-DAVIS dataset
wget https://storage.googleapis.com/dm-tapnet/tapvid_davis.zip
unzip tapvid_davis.zip

# TAP-Vid-RGB-Stacking dataset
wget https://storage.googleapis.com/dm-tapnet/tapvid_rgb_stacking.zip
unzip tapvid_rgb_stacking.zip

# RoboTAP dataset
wget https://storage.googleapis.com/dm-tapnet/robotap/robotap.zip
unzip robotap.zip
```
For downloading TAP-Vid-Kinetics, please refer to official [TAP-Vid repository](https://github.com/google-deepmind/tapnet/tree/main/tapnet/tapvid).

---

## å®éªŒç»“æœï¼ˆEfficientNet ä¸åŒç‰ˆæœ¬å¯¹æ¯”ï¼‰
| EfficientNetç‰ˆæœ¬ | æ•°æ®é›† | ç²¾åº¦ | é€Ÿåº¦ | å¤‡æ³¨ |
|------------------|--------|------|------|------|
| b0               |        |      |      |      |
| b2               |        |      |      |      |
| b5               |        |      |      |      |

ï¼ˆè¯·åœ¨æ­¤å¤„è¡¥å……å…·ä½“å®éªŒç»“æœï¼‰

---

## ONNX å¯¼å‡ºä¸è½¬æ¢æ³¨æ„äº‹é¡¹

**å‚è€ƒè„šæœ¬**ï¼šæœ¬é¡¹ç›®æä¾›äº†ä¸€ä¸ªå°†æ¨¡å‹è½¬æ¢ä¸º ONNX æ ¼å¼çš„ç¤ºä¾‹è„šæœ¬ï¼Œè¯¦æƒ…è¯·è§ **[convert_onxx.py](https://github.com/zyp-up/locotrack-pytorch-efficientnet/blob/main/convert_onxx.py)**ã€‚

1. **grid_sample å…¼å®¹æ€§**ï¼šåŸç‰ˆ LocoTrack ä½¿ç”¨äº† 5D çš„ `grid_sample`ï¼Œä½† ONNX v20 åŠä»¥ä¸‹ç‰ˆæœ¬ä»…æ”¯æŒ 4Dã€‚è§£å†³æ–¹æ¡ˆæ˜¯å°† 5D æ“ä½œæ‹†åˆ†ä¸ºå¤šæ¬¡ 4D æ“ä½œï¼Œå…·ä½“å¦‚ä¸‹ï¼š
**map_coordinates_3d ä¿®æ”¹**ï¼šéœ€å°† `map_coordinates_3d` å‡½æ•°å®ç°ä¸ºåˆ†æ­¥ 2D grid_sample + 1D çº¿æ€§æ’å€¼ï¼Œå…·ä½“ä»£ç å¯å‚è€ƒä¸‹æ–¹ç¤ºä¾‹ï¼š

   ```python
   def map_coordinates_3d(
       feats: torch.Tensor, coordinates: torch.Tensor
   ) -> torch.Tensor:
       """Maps 3D coordinates to corresponding features using bilinear interpolation."""
   
       # --- Modified Code for ONNX Export Compatibility ---
       # This modification replaces the 5D grid_sample with a 4D grid_sample
       # followed by a manual 1D linear interpolation, which is supported by ONNX exporter.
   
       B, _, _, _, C = feats.shape
       x = feats.permute(0, 4, 1, 2, 3)  # Shape: (B, C, W, H, D)
       B, C, W, H, D = x.shape
       N = coordinates.shape
   
       # Prepare coordinates, same as original
       y = coordinates[:, :, None, None, :].float()
       y[..., 0] = y[..., 0] + 0.5
       # Use float tensor for division
       shape_tensor = torch.tensor(x.shape[2:], device=y.device, dtype=torch.float32)
       y = 2 * (y / shape_tensor) - 1
       y = torch.flip(y, dims=(-1,))  # Coords are now in (D, H, W) order
   
       # Part 1: Perform 2D grid_sample over W and H for ALL D slices simultaneously
       y_hw = y[..., 1:]  # Shape: (B, N, 1, 1, 2), containing (H, W) coords
   
       # Reshape x from (B, C, W, H, D) to a "batch" of 2D images (B*D, C, W, H)
       x_perm = x.permute(0, 4, 1, 2, 3)  # Shape: (B, D, C, W, H)
       x_4d = x_perm.reshape(B * D, C, W, H)
   
       # Reshape grid to match the new batch of 2D images
       y_hw_grid = y_hw.permute(0, 1, 3, 2, 4).squeeze(2) # (B, N, 1, 2)
       y_hw_grid = y_hw_grid.expand(-1, -1, D, -1) # (B, N, D, 2)
       y_hw_grid = y_hw_grid.permute(0, 2, 1, 3).reshape(B * D, N, 1, 2) # (B*D, N, 1, 2)
   
       # Perform 2D sampling
       sampled_2d = F.grid_sample(
           x_4d, y_hw_grid, mode='bilinear', align_corners=False, padding_mode='border'
       )  # Shape: (B*D, C, N, 1)
   
       # Reshape back to get a tensor with a separate D dimension
       # Shape: (B, D, C, N) -> (B, C, N, D)
       sampled_all_d = sampled_2d.reshape(B, D, C, N).permute(0, 2, 3, 1)
   
       # Part 2: Manually perform linear interpolation along the D dimension
       y_d_norm = y[..., 0].squeeze(-1).squeeze(-1)  # Shape: (B, N), normalized coords [-1, 1]
   
       # Un-normalize D coordinate to a float index in range [0, D-1]
       d_coord = (y_d_norm + 1) / 2 * (D - 1)
   
       # Get the integer indices for lower and upper bounds
       d0 = torch.floor(d_coord).long().clamp(0, D - 2)
       d1 = d0 + 1
   
       # Gather the values from the lower and upper bounds
       # view/expand are used to create the correct indexing shape
       d0_idx = d0.view(B, 1, N, 1).expand(-1, C, -1, -1)
       V0 = sampled_all_d.gather(3, d0_idx).squeeze(3)  # Shape: (B, C, N)
   
       d1_idx = d1.view(B, 1, N, 1).expand(-1, C, -1, -1)
       V1 = sampled_all_d.gather(3, d1_idx).squeeze(3)  # Shape: (B, C, N)
   
       # Calculate interpolation weights
       w1 = (d_coord - d0).view(B, 1, N)  # Shape: (B, 1, N)
       w0 = 1 - w1
   
       # Perform the interpolation
       interpolated_val = w0 * V0 + w1 * V1  # Shape: (B, C, N)
   
       # Permute to the final desired shape (B, N, C)
       out = interpolated_val.permute(0, 2, 1)
   
       return out
   ```

3. **Squeeze æ“ä½œ**ï¼šå¯¹äºONNXçš„ä»»ä½•ç‰ˆæœ¬ï¼Œéƒ½ä¸é€‚åˆåœ¨ä»£ç ä¸­å‡ºç°ä¸€æ¬¡æ€§squeezeå¤šä¸ªç»´åº¦ã€‚å¦‚æœä»£ç ä¸­ä¸€æ¬¡æ€§å‡ºç°å¤šä¸ªç»´åº¦ï¼Œéœ€è¦å°†æ“ä½œå˜ä¸ºè¿ç»­squeezeå•ä¸ªç»´åº¦ï¼Œä¾‹å¦‚ `.squeeze(3,4)` åº”ä¿®æ”¹ä¸º `.squeeze(4).squeeze(3)`ã€‚

4. **å…¶ä»–ç®—å­æ”¯æŒ**ï¼šéƒ¨åˆ† PyTorch ç®—å­åœ¨ ONNX å¯¼å‡ºæ—¶å¯èƒ½ä¸è¢«æ”¯æŒï¼Œå»ºè®®æå‰æŸ¥é˜… ONNX æ–‡æ¡£å¹¶è¿›è¡Œç®—å­æ›¿æ¢æˆ–è‡ªå®šä¹‰å¯¼å‡ºã€‚


å¦‚éœ€è¯¦ç»†ä»£ç ç¤ºä¾‹æˆ–é‡åˆ°å…·ä½“é—®é¢˜ï¼Œæ¬¢è¿éšæ—¶äº¤æµã€‚

## Training Dataset Preparation
Download the panning-MOVi-E dataset used for training (approximately 273GB) from Huggingface using the following script. Git LFS should be installed to download the dataset. To install Git LFS, please refer to this [link](https://docs.github.com/en/repositories/working-with-files/managing-large-files/installing-git-large-file-storage?platform=linux). Additionally, downloading instructions for the Huggingface dataset are available at this [link](https://huggingface.co/docs/hub/en/datasets-downloading)
```bash
git clone git@hf.co:datasets/hamacojr/LocoTrack-panning-MOVi-E
```

## ğŸ“š Citing this Work
Please use the following bibtex to cite our work:
```
@article{cho2024local,
  title={Local All-Pair Correspondence for Point Tracking},
  author={Cho, Seokju and Huang, Jiahui and Nam, Jisu and An, Honggyu and Kim, Seungryong and Lee, Joon-Young},
  journal={arXiv preprint arXiv:2407.15420},
  year={2024}
}
```

## ğŸ™ Acknowledgement
This project is largely based on the [LocoTrack repository](https://github.com/cvlab-kaist/locotrack). Thanks to the authors for their invaluable work and contributions.